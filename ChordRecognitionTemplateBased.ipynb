{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Homework #2 - Template Based Chord Recognition</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import librosa\n",
    "import pandas\n",
    "import libfmp.b\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Configurations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_rate=44100\n",
    "window_length=4096\n",
    "hop_size=2048\n",
    "norm_input='2'\n",
    "norm_output='max'\n",
    "norm_threshold=0.0001\n",
    "smooth_filter_length=41\n",
    "smooth_filter_window_type='hanning'\n",
    "down_sampling_factor=10\n",
    "feature_normalization=True\n",
    "feature_compression=True\n",
    "feature_smoothing=True\n",
    "feature_downsample=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Features processing functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compress_feature_sequence(feature_sequence, gamma=0.1):\n",
    "    \"\"\" TODO\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    compressed_feature_sequence = np.log(1 + gamma * np.abs(feature_sequence) ** 2)\n",
    "    return compressed_feature_sequence\n",
    "\n",
    "def normalize_feature_sequence(feature_sequence, norm='2', v=None):\n",
    "    \"\"\"Normalizes the columns of a feature sequence\n",
    "\n",
    "    Args:\n",
    "        feature_sequence: Feature sequence\n",
    "        v: Used instead of normalization below `norm_threshold`. If None, uses unit vector for given norm\n",
    "\n",
    "    Returns:\n",
    "        feature_sequence_norm: Normalized feature sequence\n",
    "    \"\"\"\n",
    "\n",
    "    K, N = feature_sequence.shape\n",
    "    feature_sequence_norm = np.zeros((K, N))\n",
    "\n",
    "    if v is None:\n",
    "        v = np.ones(K, dtype=np.float64)\n",
    "\n",
    "    if norm == '1':\n",
    "        for n in range(N):\n",
    "            s = np.sum(np.abs(feature_sequence[:, n]))\n",
    "            if s > norm_threshold:\n",
    "                feature_sequence_norm[:, n] = feature_sequence[:, n] / s\n",
    "            else:\n",
    "                feature_sequence_norm[:, n] = v / K\n",
    "\n",
    "    if norm == '2':\n",
    "        for n in range(N):\n",
    "            s = np.sqrt(np.sum(feature_sequence[:, n] ** 2))\n",
    "            if s > norm_threshold:\n",
    "                feature_sequence_norm[:, n] = feature_sequence[:, n] / s\n",
    "            else:\n",
    "                feature_sequence_norm[:, n] = v / np.sqrt(K)\n",
    "\n",
    "    if norm == 'max':\n",
    "        for n in range(N):\n",
    "            s = np.max(np.abs(feature_sequence[:, n]))\n",
    "            if s > norm_threshold:\n",
    "                feature_sequence_norm[:, n] = feature_sequence[:, n] / s\n",
    "            else:\n",
    "                feature_sequence_norm[:, n] = v\n",
    "\n",
    "    return feature_sequence_norm\n",
    "\n",
    "def smooth_feature_sequence(feature_sequence):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: Feature sequence\n",
    "        Fs: Frame rate of `X`\n",
    "        filt_len: Length of smoothing filter\n",
    "        down_sampling: Downsampling factor\n",
    "        w_type: Window type of smoothing filter\n",
    "\n",
    "    Returns:\n",
    "        X_smooth: Smoothed and downsampled feature sequence\n",
    "        Fs_feature: Frame rate of `X_smooth`\n",
    "    \"\"\"\n",
    "\n",
    "    filter_kernel = signal.get_window(smooth_filter_window_type, smooth_filter_length)\n",
    "    # use expand dims to add one dimension to the window, from (L, ) to (1,L)\n",
    "    expanded_filter_kernel = np.expand_dims(filter_kernel, axis=0)\n",
    "    # mode='same' guarantees that the final length of the convolution is equal to the length of feature_sequence\n",
    "    smoothed_feature_sequence = signal.convolve(feature_sequence, expanded_filter_kernel, mode='same') / smooth_filter_length\n",
    "    return smoothed_feature_sequence\n",
    "\n",
    "def downsample_feature_sequence(feature_sequence, feature_rate):\n",
    "    \"\"\" TODO\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    downsampled_feature_sequence = feature_sequence[:, ::down_sampling_factor]\n",
    "    downsampled_fs = feature_rate / down_sampling_factor\n",
    "    return downsampled_feature_sequence, downsampled_fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Template-based chord recognition steps</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_audio(wav_file_path: str):\n",
    "    \"\"\" Load WAV audio file from a system path\n",
    "\n",
    "    Args:\n",
    "        wav_file_path (str): System path to a WAV file\n",
    "\n",
    "    Returns:\n",
    "        audio_file: (np.ndarray): Audio signal\n",
    "        audio_duration (float): Duration in seconds of the audio signal\n",
    "    \"\"\"\n",
    "\n",
    "    audio_file, Fs = librosa.load(wav_file_path, sr=sample_rate)\n",
    "    audio_duration = audio_file.shape[0] / sample_rate\n",
    "\n",
    "    return audio_file, audio_duration\n",
    "\n",
    "def chroma_representation(audio_file):\n",
    "    \"\"\" TODO\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Chromagram\n",
    "        Fs_X (scalar): Feature reate of chromagram\n",
    "        x (np.ndarray): Audio signal\n",
    "        Fs (scalar): Sampling rate of audio signal\n",
    "        x_dur (float): Duration (seconds) of audio signal\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute chroma features with STFT\n",
    "    chroma_features = librosa.stft(audio_file, n_fft=window_length, hop_length=hop_size, pad_mode='constant', center=True)\n",
    "    chroma_features = np.abs(chroma_features) ** 2\n",
    "\n",
    "    if feature_compression:\n",
    "        compress_feature_sequence(chroma_features)\n",
    "\n",
    "    chroma_features = librosa.feature.chroma_stft(S=chroma_features, sr=sample_rate, tuning=0, norm=None, hop_length=hop_size, n_fft=window_length)\n",
    "    chroma_feature_rate = sample_rate / hop_size\n",
    "\n",
    "    return chroma_features, chroma_feature_rate\n",
    "\n",
    "def generate_triads_templates():\n",
    "    \"\"\"Generate chord templates of major and minor triads\n",
    "\n",
    "    Returns:\n",
    "        chord_templates (np.ndarray): Matrix containing chord_templates as columns\n",
    "    \"\"\"\n",
    "\n",
    "    template_cmaj = np.array([[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]]).T\n",
    "    template_cmin = np.array([[1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]]).T\n",
    "    c_triads_templates = np.concatenate((template_cmaj, template_cmin), axis=1)\n",
    "    num_chord = 12 * c_triads_templates.shape[1]\n",
    "    triads_templates = np.ones((12, num_chord))\n",
    "    for shift in range(12):\n",
    "        shifted_templates = np.roll(c_triads_templates, shift, axis=0)\n",
    "        triads_templates[:, shift::12] = shifted_templates\n",
    "\n",
    "    return triads_templates\n",
    "\n",
    "def generate_chord_labels():\n",
    "    \"\"\"Generate chord labels for major and minor triads\n",
    "\n",
    "    Returns:\n",
    "        chord_labels (list): List of chord labels\n",
    "    \"\"\"\n",
    "    chroma_labels = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "    chord_labels_maj = chroma_labels\n",
    "    chord_labels_min = [s + 'm' for s in chroma_labels]\n",
    "    chord_labels = chord_labels_maj + chord_labels_min\n",
    "    return chord_labels\n",
    "\n",
    "def pre_processing(chroma_features, chroma_feature_rate, triads_templates):\n",
    "    \"\"\" TODO\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    processed_chroma_feature = chroma_features\n",
    "    processed_triads_templates = triads_templates\n",
    "\n",
    "    if feature_normalization:\n",
    "        processed_chroma_feature = normalize_feature_sequence(processed_chroma_feature, norm=norm_input)\n",
    "        processed_triads_templates = normalize_feature_sequence(triads_templates, norm=norm_input)\n",
    "\n",
    "    if feature_smoothing:\n",
    "        processed_chroma_feature = smooth_feature_sequence(processed_chroma_feature)\n",
    "\n",
    "    if feature_downsample:\n",
    "        processed_chroma_feature, downsampled_rate = downsample_feature_sequence(processed_chroma_feature, chroma_feature_rate)\n",
    "        return processed_chroma_feature, processed_triads_templates, downsampled_rate\n",
    "\n",
    "    return processed_chroma_feature, processed_triads_templates\n",
    "\n",
    "def pattern_matching(chroma_features, triads_template):\n",
    "    \"\"\" TODO\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    chord_similarity = np.matmul(triads_template.T, chroma_features)\n",
    "\n",
    "    return chord_similarity\n",
    "\n",
    "def post_processing(chord_similarity):\n",
    "    \"\"\" TODO\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    processed_chord_similarity = chord_similarity\n",
    "\n",
    "    if feature_normalization:\n",
    "        processed_chord_similarity = normalize_feature_sequence(processed_chord_similarity, norm=norm_output)\n",
    "\n",
    "    return processed_chord_similarity\n",
    "\n",
    "def recognition_result(chord_similarity, triads_templates):\n",
    "    \"\"\" TODO\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    chord_labels = generate_chord_labels()\n",
    "    chord_max = (chord_similarity == chord_similarity.max(axis=0))\n",
    "    chord_labels_indexes = np.arange(24)\n",
    "    recognized_chord_labels = []\n",
    "    for i in range(chord_max.shape[1]):\n",
    "        recognized_chord_label_index = chord_labels_indexes[chord_max[:, i]][0]\n",
    "        recognized_chord_labels = recognized_chord_labels + [chord_labels[recognized_chord_label_index]]\n",
    "\n",
    "    return recognized_chord_labels, chord_max, chord_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Template-based chord recognition implementation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_template_based_chord_recognition(audio_file_path):\n",
    "    \"\"\" TODO\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Load audio file\n",
    "    audio_file, audio_duration = load_audio(audio_file_path)\n",
    "\n",
    "    # Compute chromagram\n",
    "    chroma_features, chroma_feature_rate = chroma_representation(audio_file)\n",
    "\n",
    "    # Generate triads template\n",
    "    triads_template = generate_triads_templates()\n",
    "\n",
    "    # Pre-processing\n",
    "    if feature_downsample:\n",
    "        chroma_features, triads_template, chroma_feature_rate = pre_processing(chroma_features, chroma_feature_rate, triads_template)\n",
    "    else:\n",
    "        chroma_features, triads_template = pre_processing(chroma_features, chroma_feature_rate, triads_template)\n",
    "\n",
    "    # Pattern matching\n",
    "    chord_similarity = pattern_matching(chroma_features, triads_template)\n",
    "\n",
    "    # Post-processing\n",
    "    chord_similarity = post_processing(chord_similarity)\n",
    "\n",
    "    # Chord recognition\n",
    "    recognized_chord_labels, chord_max, chord_labels = recognition_result(chord_similarity, triads_template)\n",
    "\n",
    "    return recognized_chord_labels, chord_max, chord_labels, chord_similarity, chroma_features, chroma_feature_rate, triads_template, audio_file, audio_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Perform template-based chord recognition and plot results</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Perform chords recognition\n",
    "audio_file_path = os.path.join('data', 'wav', 'Beatles_LetItBe.wav')\n",
    "recognized_chord_labels, chord_max, chord_labels, chord_similarity, chroma_features, chroma_feature_rate, triads_template, audio_file, audio_duration = compute_template_based_chord_recognition(audio_file_path)\n",
    "\n",
    "# Annotations\n",
    "time_frames_number = len(recognized_chord_labels)\n",
    "time_axis = np.arange(time_frames_number) / chroma_feature_rate\n",
    "chord_annotation_seconds = []\n",
    "for i in range(1, time_frames_number):\n",
    "    start = time_axis[i - 1]\n",
    "    end = time_axis[i]\n",
    "    chord_annotation_seconds = chord_annotation_seconds + [(start, end, recognized_chord_labels[i - 1])]\n",
    "\n",
    "color_ann = {'C': [1, 0.5, 0, 1], 'G': [0, 1, 0, 1], 'Am': [1, 0, 0, 1], 'F': [0, 0, 1, 1], 'Em': [1, 0.5, 1, 0.5]}\n",
    "# Figure configuration\n",
    "cmap = libfmp.b.compressed_gray_cmap(alpha=1, reverse=False)\n",
    "fig, ax = plt.subplots(4, 2, figsize=(20, 24), gridspec_kw={'width_ratios': [1, 0.03], 'height_ratios': [1.5, 3, 3, 3]})\n",
    "\n",
    "# Plot audio signal\n",
    "libfmp.b.plot_signal(audio_file, sample_rate, ax=ax[0,0], title='Audio signal')\n",
    "libfmp.b.plot_segments_overlay(chord_annotation_seconds, ax=ax[0,0], time_max=audio_duration, print_labels=False, colors=color_ann, alpha=0.1)\n",
    "ax[0,1].axis('off')\n",
    "\n",
    "# PLot chromagram\n",
    "title = 'STFT-based chromagram (feature rate = %0.1f Hz)' % chroma_feature_rate\n",
    "libfmp.b.plot_chromagram(chroma_features, ax=[ax[1,0], ax[1,1]], Fs=chroma_feature_rate, clim=[0, 1], xlabel='', title=title)\n",
    "libfmp.b.plot_segments_overlay(chord_annotation_seconds, ax=ax[1,0], time_max=audio_duration,  print_labels=False, colors=color_ann, alpha=0.1)\n",
    "\n",
    "# Plot similarity matrix\n",
    "title = 'Time–chord representation of chord similarity matrix'\n",
    "libfmp.b.plot_matrix(chord_similarity, ax=[ax[2, 0], ax[2, 1]], Fs=chroma_feature_rate, title=title, ylabel='Chord', xlabel='')\n",
    "ax[2, 0].set_yticks(np.arange(len(chord_labels)))\n",
    "ax[2, 0].set_yticklabels(chord_labels)\n",
    "libfmp.b.plot_segments_overlay(chord_annotation_seconds, ax=ax[2, 0], time_max=audio_duration, print_labels=False, colors=color_ann, alpha=0.1)\n",
    "\n",
    "# Plot chord recognition results\n",
    "title = 'Time–chord representation of chord recognition result'\n",
    "libfmp.b.plot_matrix(chord_max, ax=[ax[3, 0], ax[3, 1]], Fs=chroma_feature_rate, title=title, ylabel='Chord', xlabel='')\n",
    "ax[3, 0].set_yticks(np.arange(len(chord_labels)))\n",
    "ax[3, 0].set_yticklabels(chord_labels)\n",
    "ax[3, 0].grid()\n",
    "libfmp.b.plot_segments_overlay(chord_annotation_seconds, ax=ax[3, 0], time_max=audio_duration, print_labels=False, time_label='Time (seconds)', colors=color_ann, alpha=0.1)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Ground truth processing functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv(csv_file_path):\n",
    "\n",
    "    ground_truth_csv = pandas.read_csv(csv_file_path, sep=',', keep_default_na=False, header=0)\n",
    "\n",
    "    segment_annotation_indices = []\n",
    "    for i, (start, end, label) in ground_truth_csv.iterrows():\n",
    "        start_index = int(np.round(start * chroma_feature_rate))\n",
    "        end_index = int(np.round(end * chroma_feature_rate))\n",
    "        segment_annotation_indices = segment_annotation_indices + [(start_index, end_index, label)]\n",
    "\n",
    "    return segment_annotation_indices\n",
    "\n",
    "def convert_segment_annotations(segment_annotation_indices):\n",
    "\n",
    "    frame_labels_sequence = []\n",
    "    for segment in segment_annotation_indices:\n",
    "        segment_indices_count = segment[1] - segment[0]\n",
    "        for k in range(segment_indices_count):\n",
    "            frame_labels_sequence.append(segment[2])\n",
    "\n",
    "    # Pad frame label sequence to match number of frames if needed\n",
    "    frame_labels_sequence_padding = chord_max.shape[1] - len(frame_labels_sequence)\n",
    "    for i in range(frame_labels_sequence_padding):\n",
    "        frame_labels_sequence.append(frame_labels_sequence[-1])\n",
    "\n",
    "    return frame_labels_sequence\n",
    "\n",
    "def get_binary_time_chord_matrix(labels_sequence):\n",
    "\n",
    "    time_chord_matrix = np.zeros((len(chord_labels), len(labels_sequence)))\n",
    "    for i in range(time_chord_matrix.shape[1]):\n",
    "        chord_label = labels_sequence[i]\n",
    "        if chord_label in chord_labels:\n",
    "            label_index = chord_labels.index(chord_label)\n",
    "            time_chord_matrix[label_index, i] = 1\n",
    "\n",
    "    return time_chord_matrix\n",
    "\n",
    "def normalize_chord_labels(chord_labels):\n",
    "    \"\"\"Replace for segment-based annotation in each chord label the string ':min' by 'm'\n",
    "    and convert flat chords into sharp chords using enharmonic equivalence\n",
    "\n",
    "    Args:\n",
    "        ann (list): Segment-based annotation with chord labels\n",
    "\n",
    "    Returns:\n",
    "        ann_conv (list): Converted segment-based annotation with chord labels\n",
    "    \"\"\"\n",
    "\n",
    "    normalized_chord_labels = copy.deepcopy(chord_labels)\n",
    "\n",
    "    for i in range(len(chord_labels)):\n",
    "        normalized_chord_labels[i] = normalized_chord_labels[i].replace('Db', 'C#')\n",
    "        normalized_chord_labels[i] = normalized_chord_labels[i].replace('Eb', 'D#')\n",
    "        normalized_chord_labels[i] = normalized_chord_labels[i].replace('Fb', 'E')\n",
    "        normalized_chord_labels[i] = normalized_chord_labels[i].replace('Gb', 'F#')\n",
    "        normalized_chord_labels[i] = normalized_chord_labels[i].replace('Ab', 'G#')\n",
    "        normalized_chord_labels[i] = normalized_chord_labels[i].replace('Bb', 'A#')\n",
    "        normalized_chord_labels[i] = normalized_chord_labels[i].replace('Cb', 'B')\n",
    "        normalized_chord_labels[i] = re.sub(r':(min|hdim|dim)\\d{0,2}\\(?[b#]?\\d{0,2}\\)?', 'm', normalized_chord_labels[i])\n",
    "        normalized_chord_labels[i] = re.sub(r':(maj|sus)?\\d{0,2}\\(?[b#]?\\d{0,2}\\)?', '', normalized_chord_labels[i])\n",
    "        normalized_chord_labels[i] = re.sub(r':?\\/[b#]?\\d{0,2}', '', normalized_chord_labels[i])\n",
    "\n",
    "    return normalized_chord_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Ground truth reading implementation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_ground_truth(csv_file_path):\n",
    "    \"\"\"Convert segment-based chord annotation into various formats\n",
    "\n",
    "    Args:\n",
    "        csv_file_path (str): Filename of segment-based chord annotation\n",
    "\n",
    "    Returns:\n",
    "        ann_matrix (np.ndarray): Encoding of label sequence in form of a binary time-chord representation\n",
    "        ann_frame (list): Label sequence (specified on the frame level)\n",
    "    \"\"\"\n",
    "\n",
    "    segment_annotation_indices = read_csv(csv_file_path)\n",
    "\n",
    "    ground_truth_chord_labels = convert_segment_annotations(segment_annotation_indices)\n",
    "\n",
    "    normalized_ground_truth_chord_labels = normalize_chord_labels(ground_truth_chord_labels)\n",
    "\n",
    "    ground_truth_matrix = get_binary_time_chord_matrix(normalized_ground_truth_chord_labels)\n",
    "\n",
    "    return normalized_ground_truth_chord_labels, ground_truth_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Perform ground truth reading</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Perform ground truth reading\n",
    "csv_file_path = os.path.join('data', 'csv', 'Beatles_LetItBe.csv')\n",
    "ground_truth_chord_labels, ground_truth_matrix = read_ground_truth(csv_file_path)\n",
    "\n",
    "# Annotations\n",
    "time_frames_number = len(ground_truth_chord_labels)\n",
    "time_axis = np.arange(time_frames_number) / chroma_feature_rate\n",
    "chord_annotation_seconds = []\n",
    "for i in range(1, time_frames_number):\n",
    "    start = time_axis[i - 1]\n",
    "    end = time_axis[i]\n",
    "    chord_annotation_seconds = chord_annotation_seconds + [(start, end, ground_truth_chord_labels[i - 1])]\n",
    "\n",
    "# Plot\n",
    "cmap = libfmp.b.compressed_gray_cmap(alpha=1, reverse=False)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8), gridspec_kw={'width_ratios': [1, 0.03], 'height_ratios': [2]})\n",
    "\n",
    "# Plot chord recognition results\n",
    "title='Time–chord representation of reference annotations'\n",
    "libfmp.b.plot_matrix(ground_truth_matrix, ax=[ax[0], ax[1]], Fs=chroma_feature_rate, title=title, ylabel='Chord', xlabel='')\n",
    "ax[0].set_yticks(np.arange(len(chord_labels)))\n",
    "ax[0].set_yticklabels(chord_labels)\n",
    "libfmp.b.plot_segments_overlay(chord_annotation_seconds, ax=ax[0], time_max=audio_duration, print_labels=False, time_label='Time (seconds)', colors=color_ann, alpha=0.1)\n",
    "ax[0].grid()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Metric evaluation implementation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_eval_measures(chord_max, ground_truth_matrix):\n",
    "    \"\"\"Compute evaluation measures including precision, recall, and F-measure\n",
    "\n",
    "    Notebook: C5/C5S2_ChordRec_Eval.ipynb\n",
    "\n",
    "    Args:\n",
    "        I_ref (np.ndarray): Reference set of items\n",
    "        I_est (np.ndarray): Set of estimated items\n",
    "\n",
    "    Returns:\n",
    "        P (float): Precision\n",
    "        R (float): Recall\n",
    "        F (float): F-measure\n",
    "        num_TP (int): Number of true positives\n",
    "        num_FN (int): Number of false negatives\n",
    "        num_FP (int): Number of false positives\n",
    "    \"\"\"\n",
    "    assert ground_truth_matrix.shape == chord_max.shape, \"Dimension of input matrices must agree\"\n",
    "    true_positive = np.sum(np.logical_and(ground_truth_matrix, chord_max))\n",
    "    false_positive = np.sum(chord_max > 0, axis=None) - true_positive\n",
    "    precision = 0\n",
    "    if true_positive > 0:\n",
    "        precision = true_positive / (true_positive + false_positive)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Perform metric evaluation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let It Be - Recognition precision: 59.16%\n"
     ]
    }
   ],
   "source": [
    "recognition_precision = compute_eval_measures(chord_max, ground_truth_matrix)\n",
    "print('Let It Be - Recognition precision: %s' % round(recognition_precision * 100, 2) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Compute evaluation for other files</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Here Comes The Sun\n",
    "audio_file_path = os.path.join('data', 'wav', 'Beatles_HereComesTheSun.wav')\n",
    "recognized_chord_labels, chord_max, chord_labels, chord_similarity, chroma_features, chroma_feature_rate, triads_template, audio_file, audio_duration = compute_template_based_chord_recognition(audio_file_path)\n",
    "\n",
    "csv_file_path = os.path.join('data', 'csv', 'Beatles_HereComesTheSun.csv')\n",
    "ground_truth_chord_labels, ground_truth_matrix = read_ground_truth(csv_file_path)\n",
    "\n",
    "recognition_precision = compute_eval_measures(chord_max, ground_truth_matrix)\n",
    "print('Here Comes The Sun - Recognition precision: %s' % round(recognition_precision * 100, 2) + '%')\n",
    "\n",
    "# ObLaDi ObLaDa\n",
    "audio_file_path = os.path.join('data', 'wav', 'Beatles_ObLaDiObLaDa.wav')\n",
    "recognized_chord_labels, chord_max, chord_labels, chord_similarity, chroma_features, chroma_feature_rate, triads_template, audio_file, audio_duration = compute_template_based_chord_recognition(audio_file_path)\n",
    "\n",
    "csv_file_path = os.path.join('data', 'csv', 'Beatles_ObLaDiObLaDa.csv')\n",
    "ground_truth_chord_labels, ground_truth_matrix = read_ground_truth(csv_file_path)\n",
    "\n",
    "recognition_precision = compute_eval_measures(chord_max, ground_truth_matrix)\n",
    "print('ObLaDi ObLaDa - Recognition precision: %s' % round(recognition_precision * 100, 2) + '%')\n",
    "\n",
    "# Penny Lane\n",
    "audio_file_path = os.path.join('data', 'wav', 'Beatles_PennyLane.wav')\n",
    "recognized_chord_labels, chord_max, chord_labels, chord_similarity, chroma_features, chroma_feature_rate, triads_template, audio_file, audio_duration = compute_template_based_chord_recognition(audio_file_path)\n",
    "\n",
    "csv_file_path = os.path.join('data', 'csv', 'Beatles_PennyLane.csv')\n",
    "ground_truth_chord_labels, ground_truth_matrix = read_ground_truth(csv_file_path)\n",
    "\n",
    "recognition_precision = compute_eval_measures(chord_max, ground_truth_matrix)\n",
    "print('Penny Lane - Recognition precision: %s' % round(recognition_precision * 100, 2) + '%')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}