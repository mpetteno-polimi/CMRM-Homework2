%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% fphw Assignment
% LaTeX Template
% Version 1.0 (27/04/2019)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Authors:
% Class by Felipe Portales-Oliva (f.portales.oliva@gmail.com) with template 
% content and modifications by Vel (vel@LaTeXTemplates.com)
%
% Template (this file) License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mwe}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{float}
\usepackage{amsmath}
\usepackage{hyperref}

\documentclass[
	12pt, % Default font size, values between 10pt-12pt are allowed
	%letterpaper, % Uncomment for US letter paper size
	%spanish, % Uncomment for Spanish
]{fphw}

% Template-specific packages
\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{mathpazo} % Use the Palatino font

\usepackage{graphicx} % Required for including images

\usepackage{booktabs} % Required for better horizontal rules in tables

\usepackage{listings} % Required for insertion of code

\usepackage{enumerate} % To modify the enumerate environment

\usepackage{leadsheets}

\usepackage{xcolor}

\usepackage{float}

\usepackage{caption}
\captionsetup[figure]{font=small}

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Homework \#2 - Template-Based Chord Recognition} % Assignment title

\author{Matteo Pettenò - Marco Viviani} % Students name

\date{December 23th, 2021} % Due date

\institute{Politecnico di Milano} % Institute or school name

\class{Computer Music - Representations and Models} % Course or class name

\professor{Clara Borrelli} % Professor or teacher in charge of the assignment

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Output the assignment title, created automatically using the information in the custom commands above

%----------------------------------------------------------------------------------------
%	ASSIGNMENT CONTENT
%----------------------------------------------------------------------------------------

\color{red}

In generale: inglese da rivedere\\ 
\color{black}

\textbf{Please note:} For the complete commented code, look at the ChordRecognitionTemplateBased.ipynb file on the notebook attached.

\section*{\color{red}Question 1}

\begin{problem}
	\textbf{Implement the template based chord recognition algorithm.}
\end{problem}

\subsection*{\color{blue}Answer}

A chord recognition algorithm consists of two steps. In the first step, the given audio recording is cut into frames that are transformed into a feature vector. This is tipically done with chroma-based audio features, which contain the tonal information of the audio signal.
In the second step, pattern matching techniques are used to map each feature vector to a set of predefined chord models. The best fit determines the chord label assigned to the given frame.

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{Immagine1.png}
 \caption{Template-Based Chord Recognition Pipeline}
\end{figure}

\subsection*{Context initialization}
In this section we decided to initialize a dictionary with song library details (experiment corpus) in order to have a code that is easier to read. In this way it will also be easier to change the parameters (question 5), doing it directly from this cell.

\subsection*{Parameters configuration}
This section of the code establishes all the known parameters and values that will be used to develop the code. It will be possible to change the parameters directly from this section when needed (question 5).

\subsection*{Features processing functions}

In this section of the code all the functions that will be useful to implement features processing are defined. To improve the chord recognition results, in fact, additional enhancement techniques are applied either before the pattern matching step (referred to as prefiltering) or after/within the pattern matching step (normalization of the output):

\begin{enumerate}
	\item \verb|compress_feature_sequence| - This function takes in input the sequence of features and applies compression in a logarithmic fashion.
	\item \verb|normalize_feature_sequence| - This function normalizes the columns of a feature sequence. One normalization strategy is to choose a suitable norm  and then to replace each n-feature vector  by $x/p(x)$. The normalization procedure as described above replaces each chroma vector by its normalized version. Intuitively speaking, normalization introduces a kind of invariance to differences in dynamics or sound intensity. The normalization procedure is only possible if $p(x) \neq 0$. Also for very small values $p(x)$ which may occur in passages of silence before the actual start of the recording or during long pauses, normalization would lead to more or less random and therefore meaningless chroma value distributions. Therefore, if $p(x)$ calls below a certain threshold, the vector $x$ may be replaced by some standard vector such as a uniform vector of norm one instead of dividing by $p(x)$. In the function exactly this steps are implemented. Normalization will be used also in post filtering in order to normalize the chord similarities.
	\item \verb|smooth_feature_sequence| - For certain music retrieval applications, chromagrams may be too detailed. In particular, it may be desirable to further increase the similarity between them. This can be achieved by smoothing procedures applied in a postprocessing step. The idea is to compute for each chroma dimension a kind of local average over time. More precisely, let \(X=(x_1,x_2,...,x_N)\) be a feature sequence with $x_n \in \mathbb{R}^k$ for $\in$[1:N], and let be $w$ a rectangular window  of length L. Then we compute for each $k \in[1:N]$ a convolution between $w$ and the sequence $[x_1(k),x_2(k),...,x_N(k)]$. Assuming a centered view, we only keep the center part of length of the convolution (\textit{scipy.signal.convolve}). The result is a smoothed feature sequence of the same dimensions. For the window (also called kernel) results in a bandwise 1D convolution. Using the parameter \textit{mode='same'} enforces the centered view. As for the window , one may also use other window types such as a \textit{Hann }window. Applying temporal smoothing using a rectangular or a \textit{Hann} window can be regarded as bandwise lowpass filtering, which attenuates fast temporal fluctuations in the feature representation.
	\item \verb|downsample_feature_sequence| - Often, to increase the efficiency of subsequent processing and analysis steps, one decimates the smoothed representation by keeping only every D-th feature, where D$\in\mathbb{N}$ is a suitable constant (typically much smaller than the window length L). This decimation, which is also referred to as downsampling, reduces the feature rate by a factor D.
\end{enumerate}

\subsection*{Template-based chord recognition steps and functions}

In this cell of the code are defined all the function that will be used in order to compute the chord recognition steps:

\begin{enumerate}
	\item \verb|load_audio| - Loads the .wav file.
	\item \verb|chroma_representation| - Computes chroma features with STFT to the eventually compressed features.
	\item \verb|generate_triads_templates| - Generate chord templates of major and minor triads. This function takes no input and returns the matrix containing the chord templates as columns. Every column will be a binary vector representing the template for every minor or major chord.
	\item \verb|generate_chord_labels| - Generate a chord labels list for major and minor triads.
This function takes no inputs and generates a list where major chords are indicated only with the tonic note and minor chords with the tonic note plus the simbol "m" (minor).
	\item \verb|pre_processing| - The aim of this function is to pre-process the features used in the algorithm. With the previous defined functions it normalizes, smooths and downsamples chroma features and triads template.
	\item \verb|pattern_matching| - It computes the \textit{similarity measure} (\textit{np.matmul}) between the triads template models and the predicted chroma features,according to this formula $(x,y)=\langle x,y\rangle/(||x||*||y||)$.
	\item \verb|post_processing| - In this section is applied post-processing through normalization to chord similarity, in order to have normalized results.
	\item \verb|recognition_result| - This function takes as input the matrix of triads templates and compares the current chord with the various templates. The tamplate of the chord that maximizes the chord similarity is the correct one. \color{red}VEDI MEGLIO\color{black}
\end{enumerate}

\subsection*{Template-based chord recognition implementation}

\color{red}Review description\color{black}

While the previous cells contain the function useful to the code, in this section there is the function that implements the real algorithm: \verb |compute_template_based_chord_recognition|



The aim of this homework is to implement the template based chord recognition algorithm. For doing this, we define a function that takes as input the path to a .wav file and returns the estimated chords sequence labels:

The output is a list where each element is the predicted chord label for the time frame n.

In first place, the function loads the audio file.\\ After that, it computes chromagram where, as said before, is performed pre-processing through compression. It is also necessary to create the triads template that is useful to compare with the predicted chord.\\
As said before is necessary to apply pre-processing with the previously defined function (normalization, smoothing, downsampling).
Then, we have to compare the predicted chord through pattern matching and post-process the result.
Only at the end we can recognize the chord through maximum similarity principle.\\

\subsection*{Perform template-based chord recognition and plot results}

In this last section we perform the algorithm with the main function previous defined with the audio file \emph{Beatles LetItBe.wav}. This last part of the code plots the audio signal, the chromagram, the similarity matrix and chord recognition results.

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{1 audio signal.png}
 \caption{Audio Signal}
\end{figure}

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{1 stft chromagram.png}
 \caption{STFT-based Chromagram}
\end{figure}

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{1 chord similarity.png}
 \caption{Chord Similarity Matrix}
\end{figure}

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{1 recognition results.png}
 \caption{Chord Recognition Results}
\end{figure}

%----------------------------------------------------------------------------------------

\section*{\color{red}Question 2}

\begin{problem}
	\textbf{Write a function to load and preprocess a reference annotation (or ground truth) file, saved in \emph{CSV} format.}
\end{problem}

\subsection*{\color{blue}Answer}

In this section is asked to write a function to load and preprocess a reference annotation (or ground truth) file, saved in CSV. \\
The function should take as input the path to a CSV file and produce as output a list of ground truth chord labels, after suitable pre processing.\\
The output must be a list where each element
n is the ground truth chord label for the n-th time window.

\subsection*{Ground truth processing functions}

In this paragraph is explaied each step of the preprocessing phase, focusing in particular on the reduction strategy of the chord label set.

\begin{enumerate}
	\item \verb|read_csv| - Reads the path of the .csv file.
	\item \verb|convert_segment_annotations| - The function (required in the first point, question 2) has the important role to convert the segment-based annotation into a frame-based label sequence adapted to the feature rate used for the chroma sequence.
	\item \verb|get_binary_time_chord_matrix| - The function takes as input the sequence of lables and returns the matrix with the duration (in binary rapresentation) of the various chords. It converts the labels used in the annotation file to match the chord labels used for the chord recognition
algorithm in terms of enharmonic equivalence (i.e., Db = C\# ).
\color{red} VEDI MEGLIO \color{black}
	\item \verb|normalize_chord_labels| - Important to notice is the function that normalizes chord labels. It replaces for segment-based annotation in each chord label the string ':min' by 'm' and convert flat chords into sharp chords using enharmonic equivalence. We can also see that half diminished, diminished and minor chords are classified as minor chords with the letter 'm'. Maj, sus and slash  chords are classified as major chords (' '). This is done by using Python's \textit{regex} (regular expressions). In this way we implemented the reduction strategy of the chord label set.
\end{enumerate}

\subsection*{Ground truth reading implementation}
The role of this function is to load and preprocess a reference annotation (or ground truth) file, saved in CSV format. The function  \verb |read_ground_truth| should takes as input the path to a CSV file and produce as output a list of ground truth chord labels, after suitable pre processing. The output is a list where each n-th element is the ground truth chord label for the time window n. 



The function converts segment-based chord annotation into various formats and returns a frame by frame reference chords label sequence, the binary time-chord matrix representation of the reference chords label sequence,the original reference annotations given in seconds, the normalized reference annotations given in seconds.

\subsection*{Perform ground truth reading}
In this section the code performs ground truth reading, plotting the results of chord recognition.

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{2 reference annotations.png}
\end{figure}

%----------------------------------------------------------------------------------------

\section*{\color{red}Question 3}

\begin{problem}
	\textbf{Propose a metric for evaluating the template based chord recognition algorithm.}
\end{problem}

\subsection*{\color{blue}Answer}

In this section we define a function that takes as input the list of predicted chord labels, the list of ground truth chord labels and computes the proposed metric value: \verb |compute_eval_measures| .

A metric is a scalar number that expresses how good is the algorithm in performing the task of chord recognition.\\

We now introduce a simple evaluation measure. Recall that, given a chroma sequence \(X=(x_1,x_2,…,x_N)\) and a set \( \Lambda :=[C,C\#,…,B,Cm,Cm\#,…,Bm] \)\\

In the context of music structure analysis, we introduce evaluation measures that are used in general information retrieval. We now adapt these notions to our chord recognition scenario. First, we define the set of items to be $L=[1:N] \times \Lambda$. In particular, the non-chord label N is left unconsidered. Then:
$L_+^R^e^f := \{(n,\lambda_n^R^e^f) \in L:n\in[1:N]\}$ are the positive (or relevant items) and $L_+^E^s^t :=\{(n,\lambda_n^E^s^t) \in L:n\in[1:N]\}$

are the items estimated as positive (or retrieved items). With these notions, an item (n,$\lambda_n$) is called a true positive (TP) in the case that the label is correct (i.e., $\lambda_n$=$\lambda_n^R^e^f$). Otherwise, (n,$\lambda_n$) is called a false positive (FP) and (n,$\lambda_n^R^e^f$) a false negative (FN). All other items in I are called true negative. With these notions, one can define the standard precision (P), recall (R), and F-measure (F):

\[P=(\#TP)/(\#TP + \#FP)\]
\[R=(\#TP)/(\#TP + \#FN)\]
\[F=(2PR)/(P + R)\]

In the way we have formulated our chord recognition problem so far, we have exactly one label per frame in the reference as well as in the estimation. From this follows that \#FP=\#FN and that the definition of accuracy coincides with precision, recall, and F-measure. This is why in the function we use as measure the precision P. If \#TP=0, the precision is setted to the default value 0.

\subsection*{Metric evaluation implementation}

\verb|compute_eval_measures| -  The function takes as input the binary time-chord matrix representation of the reference chords label sequence and the binary chord similarity matrix containing maximizing chord.
It implements the metric (precision) defined previously by calculating the cardinality (\textit{np.sum}) of TPs (true positives), FPs (false positives), FNs (false negatives). If the numerator (#TP) is bigger than value 0, the precision value is calculated as defined before.


This function implements exactly the precision measure as said in the previous paragraph.

\subsection*{Perform metric evaluation}

\color{red}Improve description\color{black}

We compute the evaluation for the song "Let it be" by Beatles.
Using the STFT-based chromagram in combination with the template-based chord recognizer, we obtained a precision of F=60,27 \begin{math}\% \end{math}. In fact, as the visualization shows, there are many sudden jumps between chord labels. We could optimize this result by choosing the optimal parameters setted at the beginning, but this won't be the best result.


\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{3 let it be metrics.png}
 \caption{Let It Be - Metrics Evaluation}
\end{figure}

\begin{problem}
	\textbf{Can you imagine a musically informed strategy that weights diferently mismatch errors of
the chord recognition algorithm?}
\end{problem}

\subsection*{\color{blue}Answer}

\color{red}Manca risposta a questa domanda!\color{black}\\
Introduces a kind of context-aware postfiltering.

%----------------------------------------------------------------------------------------

\section*{\color{red}Question 4}

\begin{problem}
	\textbf{Compute the proposed metric for the remaining 3 songs.}
\end{problem}

\subsection*{\color{blue}Answer}

\color{red}Improve description\color{black}

We compute also the proposed metric for the remaining 3 songs.
Averaging these numbers on the song level yields a mean precision P=60.48\begin{math}\% \end{math}. The value P=60.27\begin{math}\% \end{math} that we found in "Let it be" is very similar to the mean; we can say that this is still due to sudden jumps between chord labels. With HMM we can obtain P=78,5\begin{math}\% \end{math}, the value one also obtains for the entire Beatles collections (180 songs). More recent data-driven chord recognition approaches achieve recognition rates between 80 and 90 percent for the Beatles collection. In general, such numbers need to be taken with caution, since they only give limited insights into the complexity of the dataset.

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{4 here comes the sun.png}
 \caption{Here comes the sun}
\end{figure}

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{4 obladi oblada.png}
 \caption{ObLaDì ObLada}
\end{figure}

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{4 penny lane.png}
 \caption{Penny Lane}
\end{figure}

%----------------------------------------------------------------------------------------

\section*{\color{red}Question 5}

\begin{problem}
	\textbf{Analyse how algorithm parameters affect the performance of the templated based chord recognition algorithm.}
\end{problem}

\subsection*{\color{blue}Answer}

The testing phase has been implemented as follows. The 3 parameters to be studied have been saved in a list, each with 3 values to be compared. For each of the three parameters, for every song, are performed chords recognition, ground truth reading and metrics evaluation. The following values are saved in the dictionary defined at the beginning of the code: precision, true positive, false positive, false negative.
This procedure is iterated over songs dictionary and at every iteration overwrite global variables are overwritten.

\subsection*{Smooth filter length} 

Applying temporal smoothing using a rectangular or a Hann window can be regarded as bandwise lowpass filtering, which attenuates fast temporal fluctuations in the feature representation. This allows the chord estimation to be much more precise and not be affected by high frequencies. \\
We consider the following vector of values [0,30,60]: we can see an increase in accuracy up to 30 and then a substantial settlement with a slight decrease. The graphs have similar trends.\\
We can deduce that around the value 30 the precision of our algorithm will be higher. However, it is not particularly high. To do this it will also be necessary to observe which are the best values for the other parameters that the algorithm is using. \\

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{5 smooth precision.png}
 \caption{Smooth filter length}
\end{figure}

\subsection*{Down sampling factor}

Often, after considering the smooth filter length, to increase the efficiency of subsequent processing and analysis steps, one decimates the smoothed representation by keeping only every H-th feature, where  \(H \in N\) is a suitable constant (typically much smaller than the window length L). This decimation, which is also referred to as downsampling, reduces the feature rate by a factor H.\\
We take into account these values: [0, 10, 100]: the more the downsampling factor is increased, the more you reduce the feature rate and the less you read the harmonic content (in larger steps). If chosen high, inconsistently with the window length, the precision decreases.
It's good to use it in combination with the smoothing filter for efficiency. In this case 10 or less can be a good value. 

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{5 down sampling precision.png}
 \caption{Down sampling factor}
\end{figure}

\subsection*{Window length}

We consider the following vector of values of window lengths [2048, 8192, 32768]:  using an analysis window with a short duration , each chroma frame contains the onsets of at most one note. Even though the sound of each note may last much longer than the notated duration, the harmonic content of each frame is dominated by only one or two notes. This explains the misclassifications and many chord label changes in the recognition result of the first setting. \\
An obvious strategy for improving the chord recognition result is to use larger window sizes. Anyway, the larger analysis windows smooth out the originally sharp transitions between different chords, which may introduce problems at chord changes.\\

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{5 window precision.png}
 \caption{Window length}
\end{figure}

%----------------------------------------------------------------------------------------

\end{document}
