%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% fphw Assignment
% LaTeX Template
% Version 1.0 (27/04/2019)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Authors:
% Class by Felipe Portales-Oliva (f.portales.oliva@gmail.com) with template 
% content and modifications by Vel (vel@LaTeXTemplates.com)
%
% Template (this file) License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mwe}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{float}
\usepackage{amsmath}
\usepackage{hyperref}

\documentclass[
	12pt, % Default font size, values between 10pt-12pt are allowed
	%letterpaper, % Uncomment for US letter paper size
	%spanish, % Uncomment for Spanish
]{fphw}

% Template-specific packages
\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{mathpazo} % Use the Palatino font

\usepackage{graphicx} % Required for including images

\usepackage{booktabs} % Required for better horizontal rules in tables

\usepackage{listings} % Required for insertion of code

\usepackage{enumerate} % To modify the enumerate environment

\usepackage{leadsheets}

\usepackage{xcolor}

\usepackage{float}

\usepackage{caption}
\captionsetup[figure]{font=small}

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Homework \#2 - Template-Based Chord Recognition} % Assignment title

\author{Matteo Pettenò - Marco Viviani} % Students name

\date{December 23th, 2021} % Due date

\institute{Politecnico di Milano} % Institute or school name

\class{Computer Music - Representations and Models} % Course or class name

\professor{Clara Borrelli} % Professor or teacher in charge of the assignment

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Output the assignment title, created automatically using the information in the custom commands above

%----------------------------------------------------------------------------------------
%	ASSIGNMENT CONTENT
%----------------------------------------------------------------------------------------


\section*{Introduzione}

In this report we analyze step by step the code implemented in the attached \emph{Jupyter Notebook} trying to answer the questions asked for this homework. Along with the reading of this report, please make sure to check out code comments in the notebook. The first cells of the notebook are used to initialize the context and the parameters of the algorithm.

\subsection*{Context initialization}
In order to optimize the code for automation and readability, in this section we initialize a dictionary that contains all the details of the songs used in the experiment (\emph{Corpus}). The chord labels are generated here too.

\subsection*{Parameters configuration}
In this cell we declare and set to a default value all the template-based chord recognition algorithm's parameters. This approach allows us to centralize the management of the algorithm's behavior, making it easier to evaluate once implemented.

\section*{\color{red}Question 1}

\begin{problem}
	\textbf{Implement the template based chord recognition algorithm.}
\end{problem}

\subsection*{\color{blue}Answer}

A chord recognition algorithm is composed by two steps. In the first step, the audio track is cut into frames that are transformed into a feature vector. This is usually done by using chroma-based audio features, which contain the tonal information of the audio signal.
In the second step, pattern matching techniques are used to map each feature vector to a set of predefined chord models. The best fit determines the chord label assigned to the given frame.

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{Immagine1.png}
 \caption{Template-Based Chord Recognition Pipeline}
\end{figure}

\subsection*{Features processing functions}

In this section of the code all the functions that will be useful to implement features processing are defined. In fact, is possible to improve chord recognition results by using some processing techniques either before the pattern matching step (prefiltering) or after the pattern matching step (normalization of the output):

\begin{enumerate}
	\item \verb|compress_feature_sequence| - This function takes in input the sequence of features and applies compression in a logarithmic fashion.
	\item \verb|normalize_feature_sequence| - This function normalizes the columns of a feature sequence. The  normalization consists in choosing a norm and then to replace each n-feature vector  by $x/p(x)$. By doing this, each chroma vector is replaced with its normalized version.  The normalization is possible only if $p(x)\neq0$, but sometimes is possible that it leads to random chroma values. This usually happens during long pauses and moments of silence, when $p(x)$ is almost equal to zero. The solution is to replace (under a certain threshold of $p(x)$) the vector $x$  by an uniform one (usually of norm one) instead of dividing by $p(x)$. In the function exactly this steps are implemented. Normalization will be used also in post filtering in order to normalize the chord similarities.
	\item \verb|smooth_feature_sequence| -  This function applies smoothing artifacts to chromagrams in the postprocessing step. In fact, if chromagrams are too detailed, it can be a problem in the following recognition phases. Let \(X=(x_1,x_2,...,x_N)\) be a feature sequence with $x_n \in \mathbb{R}^k$ for $\in$[1:N], and let be $w$ a rectangular window  of length L. Smoothing is achieved by a convolution between each feature sequence and the filter kernel. The result is a smoothed feature sequence. 
	The parameter \textit{mode='same'} centers the view of the signal. It is also possible to use other window types such as a \textit{Hann }window, which attenuates fast temporal fluctuations in the feature representation.
	\item \verb|downsample_feature_sequence| - The function increases the efficiency of the next processing steps by downsampling the feature rate of the smoothed representation by a factor D (usually smaller than the window length L).  
\end{enumerate}

\subsection*{Template-based chord recognition steps and functions}

In this cell of the code are defined all the function that will be used in order to compute the chord recognition steps:

\begin{enumerate}
	\item \verb|load_audio| - Loads the .wav file.
	\item \verb|chroma_representation| - Computes chroma features with STFT to the eventually compressed features.
	\item \verb|generate_triads_templates| - Generates chord templates of major and minor triads. This function takes no input and returns the matrix containing the chord templates as columns. Every column will be a binary vector representing the template for every minor or major chord.
	\item \verb|generate_chord_labels| - Generates a chord labels list for major and minor triads.
This function takes no inputs and generates a list where major chords are indicated only with the tonic note and minor chords with the tonic note plus the simbol "m" (minor).
	\item \verb|pre_processing| - The aim of this function is to pre-process the features used in the algorithm. With the previous defined functions it normalizes, smooths and downsamples chroma features and triads template.
	\item \verb|pattern_matching| - 
	The idea behind this function is to use as inputs the calculated chromagram and the matrix containing the chords templates as columns. The template can be seen as a set of models where each column represents a chord. We also compute  the \textit{similarity measure} indicated by $s(t_\lambda,x_n)$ and whose general formula is $(x,y)=\frac{\langle x,y\rangle}{(||x||*||y||)}$. This is done in order to compare predicted chroma features and triads template models.
	The function assigns the chord label that maximizes the similarity between the template of the chord and the template of the chromagram:  $\lambda := argmax_\lambda s(t_\lambda,x_n)$.

	\item \verb|post_processing| - In this section is applied post-processing through normalization to chord similarity, in order to have normalized results.
	\item \verb|recognition_result| - This function takes as input the matrix of triads templates and compares the current chord with the various templates. The tamplate of the chord that maximizes the chord similarity is the correct one. 
\end{enumerate}

\subsection*{Template-based chord recognition implementation}


In this section we can find the function that implements the real template based chord recognition algorithm: \verb |compute_template_based_chord_recognition|.
It takes as input the path to a .wav file and returns the estimated chords sequence labels.
At the end the output will be a list where each element is the predicted chord label for the time frame n.

In first place, the function loads the audio file.\\ After that, it computes chromagram where is performed pre-processing through compression. It is also necessary to create the triads template that is useful to compare with the predicted chord.\\
As said before is necessary to apply pre-processing with the previously defined function (normalization, smoothing, downsampling).
Then, we have to compare the predicted chord through pattern matching and post-process the result.
Only at the end we can recognize the chord through maximum similarity principle.\\

\subsection*{Perform template-based chord recognition and plot results}

In this last section we perform the algorithm with the main function previous defined with the audio file \emph{Beatles LetItBe.wav}. This last part of the code plots the audio signal, the chromagram, the similarity matrix and chord recognition results.

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{1 audio signal.png}
 \caption{Audio Signal}
\end{figure}

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{1 stft chromagram.png}
 \caption{STFT-based Chromagram}
\end{figure}

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{1 chord similarity.png}
 \caption{Chord Similarity Matrix}
\end{figure}

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{1 recognition results.png}
 \caption{Chord Recognition Results}
\end{figure}

%----------------------------------------------------------------------------------------

\section*{\color{red}Question 2}

\begin{problem}
	\textbf{Write a function to load and preprocess a reference annotation (or ground truth) file, saved in \emph{CSV} format.}
\end{problem}

\subsection*{\color{blue}Answer}

The required function should take as input the path to a CSV file and produce as output a list of ground truth chord labels.\\
The output must be a list where each element
n is the ground truth chord label for the n-th time window.\\
Before proceeding, it is advisable to first make a clarification regarding the 3 formats used in the notations.
The CSV format of the files consists in three values which are the beginning and the end of the chord (in seconds) and the label assigned to that chord.


\subsection*{Ground truth processing functions}

In this paragraph is explaied each step of the preprocessing phase, focusing in particular on the reduction strategy of the chord label set.

\begin{enumerate}
	\item \verb|read_csv| - Reads the path of the CSV file.
	\item \verb|convert_segment_annotations| - The function (required in the first point, question 2) has the important role to convert the segment-based annotation into a frame-based label sequence adapted to the feature rate used for the chroma sequence.
	\item \verb|get_binary_time_chord_matrix| - The function takes as input the sequence of lables and returns the matrix with the duration (in binary rapresentation) of the various chords. It converts the labels used in the annotation file to match the chord labels used for the chord recognition
algorithm in terms of enharmonic equivalence (i.e., Db = C\# ).
	\item \verb|normalize_chord_labels| - Important to notice is the function that normalizes chord labels. It replaces for segment-based annotation in each chord label the string ':min' by 'm' and convert flat chords into sharp chords using enharmonic equivalence. We can also see that half diminished, diminished and minor chords are classified as minor chords with the letter 'm'. Maj, sus and slash  chords are classified as major chords (' '). This is done by using Python's \textit{regex} (regular expressions). In this way we implemented the reduction strategy of the chord label set.
\end{enumerate}

\subsection*{Ground truth reading implementation}
The role of this function is to load and preprocess a reference annotation (or ground truth) file, saved in CSV format. The function  \verb |read_ground_truth| should takes as input the path to a CSV file and produce as output a list of ground truth chord labels, after suitable pre processing. The output is a list where each n-th element is the ground truth chord label for the time window n. 



The function converts segment-based chord annotation into various formats and returns a frame by frame reference chords label sequence, the binary time-chord matrix representation of the reference chords label sequence,the original reference annotations given in seconds, the normalized reference annotations given in seconds.

\subsection*{Perform ground truth reading}
In this section the code performs ground truth reading, plotting the results of chord recognition.

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{2 reference annotations.png}
\end{figure}

%----------------------------------------------------------------------------------------

\section*{\color{red}Question 3}

\begin{problem}
	\textbf{Propose a metric for evaluating the template based chord recognition algorithm.}
\end{problem}

\subsection*{\color{blue}Answer}

In this section we define a function that takes as input the list of predicted chord labels, the list of ground truth chord labels and computes the proposed metric value: \verb |compute_eval_measures| .

A metric is a value that tells us how good is the algorithm in performing chord recognition.\\
In order to define the metric we refer to a chroma sequence \(X=(x_1,x_2,…,x_N)\) and to a set \( \Lambda :=[C,C\#,…,B,Cm,Cm\#,…,Bm] \) of major and minor chords reference.\\
Considering the set product $X \times \Lambda$, the predicted label (n,$\lambda_n$) is called a true positive (TP) in the case that the label is correct (i.e. $\lambda_n$=$\lambda_n^R^e^f$). Otherwise, (n,$\lambda_n$) is called a false positive (FP) and (n,$\lambda_n^R^e^f$) a false negative (FN). All other items in I are called true negative. With these notions, one can define the standard precision (P), recall (R), and F-measure (F):

\[P=(\#TP)/(\#TP + \#FP)\]
\[R=(\#TP)/(\#TP + \#FN)\]
\[F=(2PR)/(P + R)\]

Is important to notice that \#FP=\#FN because we have one label per frame in the reference as in the estimation. This implies that the accuracy coincides with precision P, recall R, and F-measure. This is why in the function we use as measure the precision P. If \#TP=0, the precision is setted to the default value 0.

\subsection*{Metric evaluation implementation}

\verb|compute_eval_measures| -  The function takes as input the binary time-chord matrix representation of the reference chords label sequence and the binary chord similarity matrix containing maximizing chord.
It implements the metric (precision) defined previously by calculating the cardinality (\textit{np.sum}) of TPs (true positives), FPs (false positives), FNs (false negatives). If the numerator (#TP) is bigger than value 0, the precision value is calculated as defined before.


This function implements exactly the precision measure as said in the previous paragraph.

\subsection*{Perform metric evaluation}

We compute the evaluation for the song "Let it be" by Beatles.
Using the STFT-based chromagram in combination with the template-based chord recognizer, we obtained a precision of F=60,27 \begin{math}\% \end{math}. In fact, as the visualization shows, there are many sudden jumps between chord labels. We could optimize this result by choosing the optimal parameters setted at the beginning, but this won't be the best result.


\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{3 let it be metrics.png}
 \caption{Let It Be - Metrics Evaluation}
\end{figure}

\begin{problem}
	\textbf{Can you imagine a musically informed strategy that weights differently mismatch errors of
the chord recognition algorithm?}
\end{problem}

\subsection*{\color{blue}Answer}

One possible method could be the implementation of an algorithm that, unlike the template based approach, is no longer based only on the matching between spectral templates and template models. One idea could be to make the system "musically informed", ie able to take into account, for example, which chord is more or less likely to be played (or in general which takes into account the musical context). In a case like this, perhaps it would be necessary to introduce some mathematical instrument of the probabilistic field. In this regard we propose some algorithm based on transition probabilities and on Markov chains / matrices. Under these consideration we would expect a better precision in estimating the chords.

%----------------------------------------------------------------------------------------

\section*{\color{red}Question 4}

\begin{problem}
	\textbf{Compute the proposed metric for the remaining 3 songs.}
\end{problem}

\subsection*{\color{blue}Answer}

We compute also the proposed metric for the remaining 3 songs.
Averaging these numbers on the song level yields a mean precision P=60.48\begin{math}\% \end{math}. The value P=60.27\begin{math}\% \end{math} that we found in "Let it be" is very similar to the mean; we can say that this is still due to sudden jumps between chord labels. In light of what has been said in the end of question 3 and therefore knowing that there are better algorithms that take into account other more complicated aspects, the results obtained can be considered satisfactory (in relation to the method used).

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{4 here comes the sun.png}
 \caption{Here comes the sun}
\end{figure}

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{4 obladi oblada.png}
 \caption{ObLaDì ObLada}
\end{figure}

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{4 penny lane.png}
 \caption{Penny Lane}
\end{figure}

%----------------------------------------------------------------------------------------

\section*{\color{red}Question 5}

\begin{problem}
	\textbf{Analyse how algorithm parameters affect the performance of the templated based chord recognition algorithm.}
\end{problem}

\subsection*{\color{blue}Answer}

The testing phase has been implemented as follows. The 3 parameters to be studied have been saved in a list, each with 3 values to be compared. For each of the three parameters, for every song, are performed chords recognition, ground truth reading and metrics evaluation. The following values are saved in the dictionary defined at the beginning of the code: precision, true positive, false positive, false negative.
This procedure is iterated over songs dictionary and at every iteration overwrite global variables are overwritten.

\subsection*{Smooth filter length} 

Applying temporal smoothing using a rectangular or a Hann window can be regarded as bandwise lowpass filtering, which attenuates fast temporal fluctuations in the feature representation. This allows the chord estimation to be much more precise and not be affected by high frequencies. \\
We consider the following vector of values [0,30,60]: we can see an increase in accuracy up to 30 and then a substantial settlement with a slight decrease. The graphs have similar trends.\\
We can deduce that around the value 30 the precision of our algorithm will be higher. However, it is not particularly high. To do this it will also be necessary to observe which are the best values for the other parameters that the algorithm is using. \\

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{5 smooth precision.png}
 \caption{Smooth filter length}
\end{figure}

\subsection*{Down sampling factor}

Often, after considering the smooth filter length, to increase the efficiency of subsequent processing and analysis steps, one decimates the smoothed representation by keeping only every H-th feature, where  \(H \in N\) is a suitable constant (typically much smaller than the window length L). This decimation, which is also referred to as downsampling, reduces the feature rate by a factor H.\\
We take into account these values: [0, 10, 100]: the more the downsampling factor is increased, the more you reduce the feature rate and the less you read the harmonic content (in larger steps). If chosen high, inconsistently with the window length, the precision decreases.
It's good to use it in combination with the smoothing filter for efficiency. In this case 10 or less can be a good value. 

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{5 down sampling precision.png}
 \caption{Down sampling factor}
\end{figure}

\subsection*{Window length}

We consider the following vector of values of window lengths [2048, 8192, 32768]:  using an analysis window with a short duration , each chroma frame contains the onsets of at most one note. Even though the sound of each note may last much longer than the notated duration, the harmonic content of each frame is dominated by only one or two notes. This explains the misclassifications and many chord label changes in the recognition result of the first setting. \\
An obvious strategy for improving the chord recognition result is to use larger window sizes. Anyway, the larger analysis windows smooth out the originally sharp transitions between different chords, which may introduce problems at chord changes.\\

\begin{figure}[H]
 \centering
 \includegraphics[scale=1]{5 window precision.png}
 \caption{Window length}
\end{figure}

%----------------------------------------------------------------------------------------

\end{document}
